{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "ERROR: Invalid requirement: \"'git+https://git@github.com/SKTBrain/KoBERT.git@master'\"\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install 'git+https://git@github.com/SKTBrain/KoBERT.git@master'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "ERROR: Invalid requirement: \"'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer\"\n",
      "Hint: = is not a valid operator. Did you mean == ?\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "'subdirectory'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
      "��ġ ������ �ƴմϴ�.\n"
     ]
    }
   ],
   "source": [
    "%pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user0216\\OneDrive - Sejong University\\바탕 화면\\subin\\KoBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'KoBERT' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from -r requirements.txt (line 1)) (1.23.10)\n",
      "Requirement already satisfied: gluonnlp>=0.6.0 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from -r requirements.txt (line 2)) (0.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: mxnet>=1.4.0 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from -r requirements.txt (line 3)) (1.6.0)\n",
      "Collecting onnxruntime==1.8.0\n",
      "  Using cached onnxruntime-1.8.0-cp36-cp36m-win_amd64.whl (4.7 MB)\n",
      "Requirement already satisfied: sentencepiece>=0.1.6 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from -r requirements.txt (line 5)) (0.1.91)\n",
      "\n",
      "Collecting torch>=1.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "ERROR: Cannot install -r requirements.txt (line 2), -r requirements.txt (line 3) and -r requirements.txt (line 4) because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using cached torch-1.10.2-cp36-cp36m-win_amd64.whl (226.6 MB)\n",
      "Requirement already satisfied: transformers>=4.8.1 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from -r requirements.txt (line 7)) (4.8.2)\n",
      "Requirement already satisfied: protobuf in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from onnxruntime==1.8.0->-r requirements.txt (line 4)) (3.19.4)\n",
      "Collecting flatbuffers\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from onnxruntime==1.8.0->-r requirements.txt (line 4)) (1.16.6)\n",
      "Requirement already satisfied: botocore<1.27.0,>=1.26.10 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from boto3->-r requirements.txt (line 1)) (1.26.10)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from boto3->-r requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from boto3->-r requirements.txt (line 1)) (0.5.2)\n",
      "Requirement already satisfied: requests<2.19.0,>=2.18.4 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from mxnet>=1.4.0->-r requirements.txt (line 3)) (2.18.4)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from mxnet>=1.4.0->-r requirements.txt (line 3)) (0.8.4)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from torch>=1.7.0->-r requirements.txt (line 6)) (0.8)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from torch>=1.7.0->-r requirements.txt (line 6)) (4.1.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers>=4.8.1->-r requirements.txt (line 7)) (4.47.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers>=4.8.1->-r requirements.txt (line 7)) (5.3.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers>=4.8.1->-r requirements.txt (line 7)) (0.0.53)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers>=4.8.1->-r requirements.txt (line 7)) (2020.6.8)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers>=4.8.1->-r requirements.txt (line 7)) (0.10.3)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers>=4.8.1->-r requirements.txt (line 7)) (0.0.12)\n",
      "Requirement already satisfied: filelock in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers>=4.8.1->-r requirements.txt (line 7)) (3.0.12)\n",
      "Requirement already satisfied: packaging in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers>=4.8.1->-r requirements.txt (line 7)) (21.3)\n",
      "Collecting transformers>=4.8.1\n",
      "  Using cached transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1.tar.gz (220 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "Collecting transformers>=4.8.1\n",
      "  Using cached transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
      "  Using cached transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
      "  Using cached transformers-4.16.1-py3-none-any.whl (3.5 MB)\n",
      "  Using cached transformers-4.16.0-py3-none-any.whl (3.5 MB)\n",
      "  Using cached transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
      "  Using cached transformers-4.14.1-py3-none-any.whl (3.4 MB)\n",
      "  Using cached transformers-4.13.0-py3-none-any.whl (3.3 MB)\n",
      "  Using cached transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
      "  Using cached transformers-4.12.4-py3-none-any.whl (3.1 MB)\n",
      "  Using cached transformers-4.12.3-py3-none-any.whl (3.1 MB)\n",
      "  Using cached transformers-4.12.2-py3-none-any.whl (3.1 MB)\n",
      "  Using cached transformers-4.12.1-py3-none-any.whl (3.1 MB)\n",
      "  Using cached transformers-4.12.0-py3-none-any.whl (3.1 MB)\n",
      "  Using cached transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "  Using cached transformers-4.11.2-py3-none-any.whl (2.9 MB)\n",
      "  Using cached transformers-4.11.1-py3-none-any.whl (2.9 MB)\n",
      "  Using cached transformers-4.11.0-py3-none-any.whl (2.9 MB)\n",
      "  Using cached transformers-4.10.3-py3-none-any.whl (2.8 MB)\n",
      "  Using cached transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
      "  Using cached transformers-4.10.1-py3-none-any.whl (2.8 MB)\n",
      "  Using cached transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
      "  Using cached transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
      "  Using cached transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
      "  Using cached transformers-4.9.0-py3-none-any.whl (2.6 MB)\n",
      "  Using cached transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torch>=1.7.0\n",
      "  Using cached torch-1.10.1-cp36-cp36m-win_amd64.whl (226.6 MB)\n",
      "INFO: pip is looking at multiple versions of sentencepiece to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sentencepiece>=0.1.6\n",
      "  Using cached sentencepiece-0.1.97-cp36-cp36m-win_amd64.whl (1.1 MB)\n",
      "INFO: pip is looking at multiple versions of mxnet to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting mxnet>=1.4.0\n",
      "  Using cached mxnet-1.7.0.post2-py2.py3-none-win_amd64.whl (33.1 MB)\n",
      "  Using cached mxnet-1.7.0.post1-py2.py3-none-win_amd64.whl (33.0 MB)\n",
      "  Using cached mxnet-1.6.0-py2.py3-none-win_amd64.whl (26.9 MB)\n",
      "  Using cached mxnet-1.5.0-py2.py3-none-win_amd64.whl (23.5 MB)\n",
      "  Using cached mxnet-1.4.1-py2.py3-none-win_amd64.whl (21.9 MB)\n",
      "  Using cached mxnet-1.4.0.post0-py2.py3-none-win_amd64.whl (21.9 MB)\n",
      "  Using cached mxnet-1.4.0-py2.py3-none-win_amd64.whl (21.9 MB)\n",
      "INFO: pip is looking at multiple versions of gluonnlp to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting gluonnlp>=0.6.0\n",
      "  Using cached gluonnlp-0.10.0.tar.gz (344 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: cython in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from gluonnlp>=0.6.0->-r requirements.txt (line 2)) (0.29.21)\n",
      "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy>=1.16.6\n",
      "  Using cached numpy-1.16.6-cp36-cp36m-win_amd64.whl (11.9 MB)"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/SKTBrain/KoBERT.git\n",
    "%cd KoBERT\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO: pip is looking at multiple versions of mxnet to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting gluonnlp>=0.6.0\n",
      "  Using cached gluonnlp-0.9.2.tar.gz (252 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached gluonnlp-0.9.1.tar.gz (252 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.9.0.post0.tar.gz (252 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.8.3.tar.gz (236 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.8.2.tar.gz (237 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.8.1.tar.gz (236 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of sentencepiece to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of gluonnlp to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached gluonnlp-0.8.0-py3-none-any.whl\n",
      "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached gluonnlp-0.7.1.tar.gz (233 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.7.0.tar.gz (233 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.6.0.tar.gz (209 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of boto3 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting boto3\n",
      "  Using cached boto3-1.23.10-py3-none-any.whl (132 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached boto3-1.23.9-py3-none-any.whl (132 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of boto3 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of onnxruntime to determine which version is compatible with other requirements. This could take a while.\n",
      "\n",
      "The conflict is caused by:\n",
      "    onnxruntime 1.8.0 depends on numpy>=1.16.6\n",
      "    gluonnlp 0.6.0 depends on numpy\n",
      "    mxnet 1.4.0.post0 depends on numpy<1.15.0 and >=1.8.2\n",
      "    onnxruntime 1.8.0 depends on numpy>=1.16.6\n",
      "    gluonnlp 0.6.0 depends on numpy\n",
      "    mxnet 1.4.0 depends on numpy<1.15.0 and >=1.8.2\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\optimizer\\optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  Optimizer.opt_registry[name].__name__))\n"
     ]
    }
   ],
   "source": [
    "# library and package\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mxnet in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: requests<2.19.0,>=2.18.4 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from mxnet) (2.18.4)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: numpy<1.17.0,>=1.8.2 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from mxnet) (1.16.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (1.22)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2020.6.20)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (3.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gluonnlp in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (4.47.0)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from gluonnlp) (1.16.6)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from pandas) (2020.1)\n",
      "\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (0.1.91)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.8.2 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (4.8.2)\n",
      "Requirement already satisfied: requests in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers==4.8.2) (2.18.4)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers==4.8.2) (0.0.53)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers==4.8.2) (0.8)\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers==4.8.2) (4.8.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers==4.8.2) (2020.6.8)\n",
      "Requirement already satisfied: filelock in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers==4.8.2) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers==4.8.2) (0.0.12)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers==4.8.2) (5.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers==4.8.2) (4.47.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers==4.8.2) (21.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from transformers==4.8.2) (0.10.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from huggingface-hub==0.0.12->transformers==4.8.2) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from packaging->transformers==4.8.2) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from importlib-metadata->transformers==4.8.2) (3.1.0)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from requests->transformers==4.8.2) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from requests->transformers==4.8.2) (2020.6.20)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from requests->transformers==4.8.2) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from requests->transformers==4.8.2) (1.22)\n",
      "Requirement already satisfied: six in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from sacremoses->transformers==4.8.2) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from sacremoses->transformers==4.8.2) (0.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages (from sacremoses->transformers==4.8.2) (7.1.2)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.16.6Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "    Uninstalling numpy-1.16.6:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] 액세스가 거부되었습니다: 'c:\\\\users\\\\user0216\\\\anaconda3\\\\envs\\\\mxnet\\\\lib\\\\site-packages\\\\~6mpy\\\\.libs\\\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Successfully uninstalled numpy-1.16.6\n",
      "Requirement already satisfied: torch in c:\\users\\user0216\\appdata\\roaming\\python\\python36\\site-packages (1.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "%pip install mxnet\n",
    "%pip install gluonnlp pandas tqdm\n",
    "%pip install sentencepiece==0.1.91\n",
    "%pip install transformers==4.8.2\n",
    "%pip install torch\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# data load\n",
    "\n",
    "import pandas as pd\n",
    "bnk = pd.read_excel('C:/Users/user0216/OneDrive - Sejong University/바탕 화면/subin/금융/BNK금융.xlsx')\n",
    "kb = pd.read_excel('C:/Users/user0216/OneDrive - Sejong University/바탕 화면/subin/금융/KB금융.xlsx')\n",
    "ibk = pd.read_excel('C:/Users/user0216/OneDrive - Sejong University/바탕 화면/subin/금융/기업은행.xlsx')\n",
    "shinhan = pd.read_excel('C:/Users/user0216/OneDrive - Sejong University/바탕 화면/subin/금융/신한금융.xlsx')\n",
    "woori = pd.read_excel('C:/Users/user0216/OneDrive - Sejong University/바탕 화면/subin/금융/우리금융.xlsx')\n",
    "hana = pd.read_excel('C:/Users/user0216/OneDrive - Sejong University/바탕 화면/subin/금융/하나금융.xlsx')\n",
    "\n",
    "finance = bnk.append([kb,ibk,shinhan,woori,hana])\n",
    "\n",
    "fin_e = finance.loc[finance['label'] == 'E']\n",
    "fin_s = finance.loc[finance['label'] == 'S']\n",
    "fin_g = finance.loc[finance['label'] == 'G']\n",
    "fin_o = finance.loc[finance['label'] == 'O']\n",
    "fin_n = finance[finance['label'].str.contains('N')]\n",
    "fin_n['label'] = 'N'\n",
    "fin = fin_e.append([fin_e, fin_g,fin_s, fin_n, fin_o], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(fin['label'])\n",
    "fin['label'] = encoder.transform(fin['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'E', 1: 'G', 2: 'N', 3: 'O', 4: 'S'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = dict(zip(range(len(encoder.classes_)), encoder.classes_))\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = fin.drop(fin.columns[0:1],axis=1)\n",
    "fin = fin[['text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한편 장기적으로 국제 사회에 큰 위협이 될 것으로 예상되는 기후변화에 대한 대응체계도 강화해 나가고 있습니다.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_text=[]\n",
    "\n",
    "for i in fin['text']:\n",
    "    str_text.append(str(i))\n",
    "\n",
    "str_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['text','label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = str_text\n",
    "df[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (52603, 2), indices imply (30392, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-7f165a7fbc71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m--> 497\u001b[1;33m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    498\u001b[0m             )\n\u001b[0;32m    499\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m   2025\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2027\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmgr_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m                 \u001b[0mconstruction_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtot_items\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m             raise AssertionError(\n",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[1;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[0;32m   1692\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mblock_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1694\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (52603, 2), indices imply (30392, 2)"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(str_text, columns=['text'])\n",
    "\n",
    "label = pd.DataFrame(fin['label'], columns=['label'])\n",
    "\n",
    "pd.concat([df, label], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>한편 장기적으로 국제 사회에 큰 위협이 될 것으로 예상되는 기후변화에 대한 대응체계...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>기후변화의 규제 강화에 따라 투자사업들에 대한 리스크 익스포저의 변화 투자 수익률의...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그리고 이 를 통해 금융을 통한 환경보호 및 기후변화 대응을 실현하기 위한 토대를 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BNK금융그룹은 금융산업의 미래지향적 친환경 경영을 선도하는 환경경영 전략을 추진하...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>너지 절감 활동을 추진함으로써 효율적으로 환경경영을 실천하고 있습니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30387</th>\n",
       "      <td>검증은 독자들에게 다음과 같은 사항을 제공하기 위해 설계되었습니다.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30388</th>\n",
       "      <td>보고서에 담겨 있는 환경 및 사회 분야 정보가 적정하게 기술되었는지 여부 적정하게 ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30389</th>\n",
       "      <td>한국능률협회인증원의 검증기준은 한국능률협회인증원의 검증기준에 따라 검증작업을 수행하...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30390</th>\n",
       "      <td>한국능률협회인증원은 합의된 검증 범위의 보고서 내용 사실여부와 보고된 데이터 및 보...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30391</th>\n",
       "      <td>본 검증인은 하나은행이 발간한 -7년 보고서가 이해관계자 커뮤니케이션의 수단으로 적...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30392 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      한편 장기적으로 국제 사회에 큰 위협이 될 것으로 예상되는 기후변화에 대한 대응체계...      0\n",
       "1      기후변화의 규제 강화에 따라 투자사업들에 대한 리스크 익스포저의 변화 투자 수익률의...      0\n",
       "2      그리고 이 를 통해 금융을 통한 환경보호 및 기후변화 대응을 실현하기 위한 토대를 ...      0\n",
       "3      BNK금융그룹은 금융산업의 미래지향적 친환경 경영을 선도하는 환경경영 전략을 추진하...      0\n",
       "4                너지 절감 활동을 추진함으로써 효율적으로 환경경영을 실천하고 있습니다.      0\n",
       "...                                                  ...    ...\n",
       "30387              검증은 독자들에게 다음과 같은 사항을 제공하기 위해 설계되었습니다.      3\n",
       "30388  보고서에 담겨 있는 환경 및 사회 분야 정보가 적정하게 기술되었는지 여부 적정하게 ...      3\n",
       "30389  한국능률협회인증원의 검증기준은 한국능률협회인증원의 검증기준에 따라 검증작업을 수행하...      3\n",
       "30390  한국능률협회인증원은 합의된 검증 범위의 보고서 내용 사실여부와 보고된 데이터 및 보...      3\n",
       "30391  본 검증인은 하나은행이 발간한 -7년 보고서가 이해관계자 커뮤니케이션의 수단으로 적...      3\n",
       "\n",
       "[30392 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = pd.DataFrame(fin['label'], columns=['label'])\n",
    "label.reset_index(inplace=True)\n",
    "label.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "df=pd.concat([df,label], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>한편 장기적으로 국제 사회에 큰 위협이 될 것으로 예상되는 기후변화에 대한 대응체계...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>기후변화의 규제 강화에 따라 투자사업들에 대한 리스크 익스포저의 변화 투자 수익률의...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그리고 이 를 통해 금융을 통한 환경보호 및 기후변화 대응을 실현하기 위한 토대를 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BNK금융그룹은 금융산업의 미래지향적 친환경 경영을 선도하는 환경경영 전략을 추진하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>너지 절감 활동을 추진함으로써 효율적으로 환경경영을 실천하고 있습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30387</th>\n",
       "      <td>검증은 독자들에게 다음과 같은 사항을 제공하기 위해 설계되었습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30388</th>\n",
       "      <td>보고서에 담겨 있는 환경 및 사회 분야 정보가 적정하게 기술되었는지 여부 적정하게 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30389</th>\n",
       "      <td>한국능률협회인증원의 검증기준은 한국능률협회인증원의 검증기준에 따라 검증작업을 수행하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30390</th>\n",
       "      <td>한국능률협회인증원은 합의된 검증 범위의 보고서 내용 사실여부와 보고된 데이터 및 보...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30391</th>\n",
       "      <td>본 검증인은 하나은행이 발간한 -7년 보고서가 이해관계자 커뮤니케이션의 수단으로 적...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30392 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      한편 장기적으로 국제 사회에 큰 위협이 될 것으로 예상되는 기후변화에 대한 대응체계...\n",
       "1      기후변화의 규제 강화에 따라 투자사업들에 대한 리스크 익스포저의 변화 투자 수익률의...\n",
       "2      그리고 이 를 통해 금융을 통한 환경보호 및 기후변화 대응을 실현하기 위한 토대를 ...\n",
       "3      BNK금융그룹은 금융산업의 미래지향적 친환경 경영을 선도하는 환경경영 전략을 추진하...\n",
       "4                너지 절감 활동을 추진함으로써 효율적으로 환경경영을 실천하고 있습니다.\n",
       "...                                                  ...\n",
       "30387              검증은 독자들에게 다음과 같은 사항을 제공하기 위해 설계되었습니다.\n",
       "30388  보고서에 담겨 있는 환경 및 사회 분야 정보가 적정하게 기술되었는지 여부 적정하게 ...\n",
       "30389  한국능률협회인증원의 검증기준은 한국능률협회인증원의 검증기준에 따라 검증작업을 수행하...\n",
       "30390  한국능률협회인증원은 합의된 검증 범위의 보고서 내용 사실여부와 보고된 데이터 및 보...\n",
       "30391  본 검증인은 하나은행이 발간한 -7년 보고서가 이해관계자 커뮤니케이션의 수단으로 적...\n",
       "\n",
       "[30392 rows x 1 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(str_text, columns=['text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.reindex_like of                                                    text  label\n",
       "343   한편 장기적으로 국제 사회에 큰 위협이 될 것으로 예상되는 기후변화에 대한 대응체계...      0\n",
       "344   기후변화의 규제 강화에 따라 투자사업들에 대한 리스크 익스포저의 변화 투자 수익률의...      0\n",
       "345   그리고 이 를 통해 금융을 통한 환경보호 및 기후변화 대응을 실현하기 위한 토대를 ...      0\n",
       "351   BNK금융그룹은 금융산업의 미래지향적 친환경 경영을 선도하는 환경경영 전략을 추진하...      0\n",
       "353             너지 절감 활동을 추진함으로써 효율적으로 환경경영을 실천하고 있습니다.      0\n",
       "...                                                 ...    ...\n",
       "5142              검증은 독자들에게 다음과 같은 사항을 제공하기 위해 설계되었습니다.      3\n",
       "5143  보고서에 담겨 있는 환경 및 사회 분야 정보가 적정하게 기술되었는지 여부 적정하게 ...      3\n",
       "5144  한국능률협회인증원의 검증기준은 한국능률협회인증원의 검증기준에 따라 검증작업을 수행하...      3\n",
       "5145  한국능률협회인증원은 합의된 검증 범위의 보고서 내용 사실여부와 보고된 데이터 및 보...      3\n",
       "5148  본 검증인은 하나은행이 발간한 -7년 보고서가 이해관계자 커뮤니케이션의 수단으로 적...      3\n",
       "\n",
       "[30392 rows x 2 columns]>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 1 elements, new values have 2 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-80818a7f22af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#fin_re = pd.concat([df,df_label], axis=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   5285\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5286\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5287\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5288\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5289\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    662\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_len\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mold_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             raise ValueError(\n\u001b[1;32m--> 178\u001b[1;33m                 \u001b[1;34mf\"Length mismatch: Expected axis has {old_len} elements, new \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m                 \u001b[1;34mf\"values have {new_len} elements\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 1 elements, new values have 2 elements"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(str_text)\n",
    "df.columns= ['text','label']\n",
    "df_label = pd.DataFrame(fin['label'])\n",
    "#fin_re = pd.concat([df,df_label], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_train, dataset_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.to_numpy()\n",
    "dataset_test = dataset_test.to_numpy()\n",
    "\n",
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24313 6079 30392\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train),len(dataset_test), len(fin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=5,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        #valid_length 까지만 1, 나머지는 0으로 mask를 생성\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        # return_dict=False 추가\n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device), return_dict=False)\n",
    "\n",
    "        #pooler는 CLS토큰에 대한 임베딩의 결과, 단순히 첫번째 토큰에 대한 결과가 아니라 문맥을 알아내기 위한 추가 layer를 통과한다.(아마도?)\n",
    "        #임베딩의 결과에 linear layer를 통과시켜서 classification을 진행한다.\n",
    "        # print(pooler.shape) batchsize * 768\n",
    "\n",
    "        #_는 64개(max_length)의 모든 토큰에 대한 임베딩의 결과\n",
    "        #단어 임베딩을 알고 싶을 때 사용할 수 있다.\n",
    "        #_의 첫번째 값과 pooler와는 다른 값을 가지고 있는데 pooler는 _의 첫번째 값의 추가적으로 한번더 과정을 거친다.\n",
    "        #print(_.shape) batchsize * max_len * 768\n",
    "\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out) #batchsize * num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kobert_model(model_path, vocab_file, ctx=\"cpu\"):\n",
    "    bertmodel = BertModel.from_pretrained(model_path)\n",
    "    device = torch.device(ctx)\n",
    "    bertmodel.to(device)\n",
    "    bertmodel.eval()\n",
    "    vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(vocab_file,\n",
    "                                                         padding_token='[PAD]')\n",
    "    return bertmodel, vocab_b_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KoBERT.kobert_hf.kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel, vocab = get_kobert_model('skt/kobert-base-v1',tokenizer.vocab_file)\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1562"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [len(i[0]) for i in dataset_train]\n",
    "l2 = [len(i[0]) for i in dataset_test]\n",
    "max(max(l1),max(l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 64\n",
    "batch_size = 32\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from KoBERT.kobert.utils.utils import get_tokenizer\n",
    "tok_path = get_tokenizer()\n",
    "sp = SentencepieceTokenizer(tok_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not a string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-bca58955ac96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBERTDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBERTDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-101-6e902af9289a>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair)\u001b[0m\n\u001b[0;32m      4\u001b[0m             bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msent_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-101-6e902af9289a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m             bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msent_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\gluonnlp\\data\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   1296\u001b[0m             \u001b[0mtext_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1298\u001b[1;33m         \u001b[0mtokens_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1299\u001b[0m         \u001b[0mtokens_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\gluonnlp\\data\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m   1007\u001b[0m         \"\"\"\n\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1009\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\gluonnlp\\data\\transforms.py\u001b[0m in \u001b[0;36m_tokenizer\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0msub_token\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenize_wordpiece\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m                 \u001b[0msplit_tokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\gluonnlp\\data\\transforms.py\u001b[0m in \u001b[0;36m_tokenize_wordpiece\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;31m# Swig object can not be pickled when multiprocessing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentencepiece\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activate_sp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentencepiece\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\gluonnlp\\data\\transforms.py\u001b[0m in \u001b[0;36m_activate_sp\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1163\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_activate_sp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m         self.sentencepiece = SentencepieceTokenizer(self._path, self._num_best,\n\u001b[1;32m-> 1165\u001b[1;33m                                                     self._alpha)\n\u001b[0m\u001b[0;32m   1166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_tokenize_wordpiece\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\gluonnlp\\data\\transforms.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, num_best, alpha)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_best\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSentencepieceTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_best\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_alpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\gluonnlp\\data\\transforms.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    663\u001b[0m                 'in https://github.com/google/sentencepiece#installation')\n\u001b[0;32m    664\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_processor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentencepiece\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 665\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\sentencepiece.py\u001b[0m in \u001b[0;36mLoad\u001b[1;34m(self, model_file, model_proto)\u001b[0m\n\u001b[0;32m    365\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadFromSerializedProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadFromFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user0216\\anaconda3\\envs\\mxnet\\lib\\site-packages\\sentencepiece.py\u001b[0m in \u001b[0;36mLoadFromFile\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mLoadFromFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSentencePieceProcessor_LoadFromFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     def Init(self,\n",
      "\u001b[1;31mTypeError\u001b[0m: not a string"
     ]
    }
   ],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 ('mxnet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a331f86d3e532e3b1b11eb43aba8bdf18112ee806d9744dafef9aae970b2a57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
